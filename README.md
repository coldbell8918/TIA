# Knowledge distillation for multi-task learning dense prediction with task-integrated adaptor  

We propose a task-balancing adaptor-based knowledge distillation method for multi-task learning. Our method aligns prediction features via a shared adaptor and minimizes cosine similarity loss, achieving superior performance over feature-only distillation baselines.

